{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tropical-realtor",
   "metadata": {},
   "source": [
    "**Welcome to TP 1 :** Bag of words and word tokenizer\n",
    "\n",
    "------------------------------------------------------------------------------------------\n",
    "\n",
    "**Théorie derrière l'approche du sac de mots**\n",
    "\n",
    "Pour comprendre l'approche du sac de mots, commençons d'abord par l'aide d'un exemple.\n",
    "\n",
    "Supposons que nous ayons un corpus de trois phrases :\n",
    "\n",
    "    \"Il aime jouer au football\"\n",
    "    \"Tu es sorti pour jouer au tennis?\"\n",
    "    \"John et moi jouons au tennis\"\n",
    "\n",
    "Maintenant, si nous devons effectuer une classification de texte, ou toute autre tâche, sur les données ci-dessus en utilisant des techniques statistiques, nous ne pouvons pas le faire car les techniques statistiques ne fonctionnent qu'avec des chiffres. Nous devons donc convertir ces phrases en chiffres.\n",
    "\n",
    "\n",
    "La première étape à cet égard consiste à convertir les phrases de notre corpus en token ou mots individuels. Comme ci-dessous : phrase 1 : [\"IL\", \"aime\", \"jouer\", \"au\", \"football\"]\n",
    "\n",
    "\n",
    "L'étape suivante consiste à créer un dictionnaire qui contient tous les mots de notre corpus comme clés et la fréquence d'occurrence des mots comme valeurs. \n",
    "\n",
    "\n",
    "En d'autres termes, nous devons créer un histogramme des mots de notre corpus. Regardez le tableau suivant :\n",
    "\n",
    "'Il': 1, \n",
    "'aime': 1, \n",
    "'jouer': 2,\n",
    "'au': 3,\n",
    "'football': 1, \n",
    "'Tu': 1, \n",
    "'es': 1,\n",
    "'sorti': 1, \n",
    "'pour': 1, \n",
    "'tennis': 1, \n",
    "'John': 1, \n",
    "'et': 1, \n",
    "'moi': 1, \n",
    "'jouons': 1, \n",
    "'tennis': 1\n",
    "\n",
    "\n",
    "Dans le tableau ci-dessus, vous pouvez voir chaque mot de notre corpus ainsi que sa fréquence d'occurrence. Par exemple, vous pouvez voir que puisque le mot \"jouer\" se produit deux fois dans le corpus, sa fréquence est de 2.\n",
    "\n",
    "Dans notre corpus, nous n'avions que trois phrases, il est donc facile pour nous de créer un dictionnaire qui contient tous les mots. Dans les scénarios du monde réel, il y aura des millions de mots dans le dictionnaire. \n",
    "\n",
    "Certains de ces mots auront une fréquence très faible. Les mots ayant une très faible fréquence ne sont pas très utiles, c'est pourquoi ces mots sont supprimés. Une façon de supprimer les mots ayant une fréquence moindre est de trier le dictionnaire des fréquences des mots dans l'ordre décroissant de la fréquence et de filtrer ensuite les mots ayant une fréquence supérieure à un certain seuil.\n",
    "\n",
    "Pour ça on trie notre dictionnaire par fréquence décroissante, et on supprime les tokens les moins fréquents.\n",
    "\n",
    "{'au': 3,\n",
    " 'jouer': 2,\n",
    " 'Il': 1,\n",
    " 'aime': 1,\n",
    " 'football': 1,\n",
    " 'Tu': 1,\n",
    " 'es': 1,\n",
    "}\n",
    " \n",
    "          au   jouer  il   aime   football   tu   es   \n",
    " phrase1: 1      1     1     1       1       0    0\n",
    " phrase2: 1      1     0     0       0       1    1\n",
    " phrase3: 1      0     0     0       0       0    0\n",
    " \n",
    "Maintenant que nos phrases sont des vecteurs, nous pouvous les analyser avec des techniques de data science classique. \n",
    "\n",
    "\n",
    "**Let's goooo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-department",
   "metadata": {},
   "source": [
    "**Partie 1 : Arriver à executer la cellule ci-dessous**\n",
    "\n",
    "Toutes les personnes n'arrivant pas à effectuer cette tâche d'une simplicité hors norme me briserait le coeur à jamais. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "going-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "import numpy as np  \n",
    "import random  \n",
    "import string\n",
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re \n",
    "import heapq\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-graduation",
   "metadata": {},
   "source": [
    "**Partie 2 : Scrappez le texte de page wikipédia, radicalement opposée, par exemple Trump/Biden \n",
    "\n",
    "et placez les 2 urls ci dessous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "welsh-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_html1 = urllib.request.urlopen(\"https://fr.wikipedia.org/wiki/Donald_Trump\")  \n",
    "raw_html2 = urllib.request.urlopen(\"https://fr.wikipedia.org/wiki/Joe_Biden\")\n",
    "raw_html1 = raw_html1.read()\n",
    "raw_html2 = raw_html2.read()\n",
    "\n",
    "article_html1 = bs.BeautifulSoup(raw_html1, \"html.parser\")\n",
    "article_html2 = bs.BeautifulSoup(raw_html2, \"html.parser\")\n",
    "\n",
    "article_paragraphs1 = article_html1.find_all('p')\n",
    "article_paragraphs2 = article_html2.find_all('p')\n",
    "\n",
    "article_text = ''\n",
    "\n",
    "for para in article_paragraphs1 + article_paragraphs2:  \n",
    "    article_text += para.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-character",
   "metadata": {},
   "source": [
    "**Partie 3 : Dans la documentation du package nltk, cherchez la methode permettant de séparer les phrases du corpus**\n",
    "\n",
    "mettez la liste des phrases dans la variable `corpus` ci dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "mature-centre",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tibz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "corpus = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-robinson",
   "metadata": {},
   "source": [
    "On va maintenant normaliser les phrases ( enlever les majuscules, les espaces )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "favorite-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus )):\n",
    "    corpus [i] = corpus[i].lower()\n",
    "    corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
    "    corpus [i] = re.sub(r'\\s+',' ',corpus [i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-heath",
   "metadata": {},
   "source": [
    "**Partie 4 : Maintenant créez le dictionnaire des fréquences d'apparition des mots, gardez seulement les 200 plus fréquents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "authentic-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "\n",
    "\n",
    "most_freq = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-handy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "burning-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    for token in most_freq:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "demanding-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = np.asarray(sentence_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
